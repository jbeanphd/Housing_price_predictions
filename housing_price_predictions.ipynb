{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries needed for machine learning exercise of predicting housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col='Id')\n",
    "test = pd.read_csv('test.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without data in the training dataset the model will not be able to use the variable to train the model, and without data in the test dataset the model cannot use the variable to predict anything. The model should be as or more accurate if we drop variables that don't have at least some values filled in on both the training and test datasets. \n",
    "\n",
    "Identify columns with categorical data, from those identify columns that have data in both training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['Street', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'RoofStyle', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCond', 'PavedDrive', 'Fence', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Functional', 'MSZoning', 'GarageQual', 'Heating', 'SaleType', 'RoofMatl', 'Utilities', 'Exterior1st', 'HouseStyle', 'PoolQC', 'KitchenQual', 'Exterior2nd', 'MiscFeature', 'Electrical', 'Condition2']\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in train.columns if train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(train[col]) == set(test[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate SalePrice from training dataset. Split training dataset into a train and validate data-subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.SalePrice\n",
    "train.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.8, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons listed above there is a need to identify columns with no data present in our new train and validate data-subsets. The original bad columns should also be subtracted as they will never be of use in predicting the SalesPrice in the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'BldgType', 'HouseStyle', 'ExterQual', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'CentralAir', 'KitchenQual', 'FireplaceQu', 'GarageType', 'GarageFinish', 'PavedDrive', 'Fence', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['GarageCond', 'Functional', 'HeatingQC', 'ExterCond', 'GarageQual', 'Foundation', 'Heating', 'SaleType', 'RoofMatl', 'Utilities', 'Exterior1st', 'Condition1', 'Neighborhood', 'PoolQC', 'Exterior2nd', 'MasVnrType', 'RoofStyle', 'MiscFeature', 'Electrical', 'Condition2']\n",
      "\n",
      "Categorical columns left after dropping all bad columns: ['BsmtFinType1', 'Street', 'Alley', 'LotShape', 'BsmtQual', 'FireplaceQu', 'BsmtExposure', 'CentralAir', 'ExterQual', 'BsmtFinType2', 'LotConfig', 'Fence', 'GarageType', 'BsmtCond', 'GarageFinish', 'BldgType', 'SaleCondition', 'PavedDrive', 'LandContour', 'LandSlope']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols2 = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols2 = list(set(object_cols)-set(good_label_cols2))\n",
    "\n",
    "#columns that are valid between train and valid and also full and test\n",
    "good_good_cols = list(set(good_label_cols2)- set(bad_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols2)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols2)\n",
    "\n",
    "print('\\nCategorical columns left after dropping all bad columns:', good_good_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify columns with numerical data, add those to columns with categorical data present in train, validate, and test datasets, then make a copy of train and validate data-subsets with only those columns present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train.columns if \n",
    "                X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = good_good_cols + numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train[my_cols].copy()\n",
    "X_valid2 = X_valid[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical data that is missing will be replaced with a constant value (0) using SimpleImputer. The same will be done with categorical data then OneHotEncoder will transform that into numerical data. These will be packaged together as - preprocessor - . XGBRegressor will be used as the model. The model and preprocessor will be packaged as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.impute import SimpleImputer\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[('impute', SimpleImputer(strategy='constant')),\n",
    "                                          ('label', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, good_good_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = XGBRegressor(max_depth=3, randon_state=1, n_estimators = 1000, learning_rate=0.07, \n",
    "                     objective='reg:squarederror')\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model to training subset and corresponding SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/.local/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='constant',\n",
       "                                                                verbose=0),\n",
       "                                                  ['MSSubClass', 'LotFrontage',\n",
       "                                                   'LotArea', 'OverallQual',\n",
       "                                                   'OverallCond', 'YearBuilt',\n",
       "                                                   'Yea...\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0,\n",
       "                              importance_type='gain', learning_rate=0.07,\n",
       "                              max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                              missing=None, n_estimators=1000, n_jobs=1,\n",
       "                              nthread=None, objective='reg:squarederror',\n",
       "                              random_state=0, randon_state=1, reg_alpha=0,\n",
       "                              reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "                              silent=None, subsample=1, verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pipeline.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to predict SalePrice on the validate subset, then calculate how accurate those predictions were on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 14819.280581121575\n"
     ]
    }
   ],
   "source": [
    "preds = my_pipeline.predict(X_valid2)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model as is to predict SalePrice from test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = my_pipeline.predict(test[my_cols])\n",
    "\n",
    "output = pd.DataFrame({'Id': test.index, 'SalePrice': test_preds})\n",
    "output.to_csv('submission7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model to full training dataset, then predict price on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/.local/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/jonathan/.local/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "my_pipeline.fit(train[my_cols], y)\n",
    "\n",
    "my_pipeline.predict(test[my_cols])\n",
    "\n",
    "output2 = pd.DataFrame({'Id': test.index, 'SalePrice': test_preds})\n",
    "output2.to_csv('submission8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
